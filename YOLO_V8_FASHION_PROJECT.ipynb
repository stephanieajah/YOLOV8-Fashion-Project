{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OsrMpmijKsc"
      },
      "source": [
        "#**Smart Wardrobe: Integrating YOLOv8 and Large Language Models in Fashion Styling.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjyXp5ivjE9R"
      },
      "source": [
        "###1. Install YOLOv8 and Import YOLO and other Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0CYgOODJ4E7",
        "outputId": "c052ff81-753b-4ea9-e3d0-7720f418d3ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.49-py3-none-any.whl.metadata (35 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.8.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (11.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.1+cpu)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.1+cpu)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Collecting py-cpuinfo (from ultralytics)\n",
            "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.13-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.49-py3-none-any.whl (898 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m898.7/898.7 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.13-py3-none-any.whl (26 kB)\n",
            "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: py-cpuinfo, ultralytics-thop, ultralytics\n",
            "Successfully installed py-cpuinfo-9.0.0 ultralytics-8.3.49 ultralytics-thop-2.0.13\n",
            "Collecting openai\n",
            "  Downloading openai-1.57.4-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.10.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n",
            "Downloading openai-1.57.4-py3-none-any.whl (390 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m390.3/390.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (345 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m345.0/345.0 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jiter, h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 jiter-0.8.2 openai-1.57.4\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DQTAKNtc3Czu",
        "outputId": "fe579975-0d32-4942-cd51-f852bd47c7dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import yaml\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from torchvision import transforms\n",
        "import os\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import torch\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac2LgJlakXOj"
      },
      "source": [
        "###2. Model Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51ZPxVZJkmms",
        "outputId": "4277aa29-7192-4f7a-cfef-5d5125464101"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "YOLOv8n-cls summary: 99 layers, 2,719,288 parameters, 2,719,288 gradients, 4.4 GFLOPs\n"
          ]
        }
      ],
      "source": [
        "#Load the model\n",
        "model = YOLO(\"yolov8n-cls.yaml\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2LL1iX3knZO"
      },
      "source": [
        "###3. Training and testing the Model\n",
        "\n",
        "During training, YOLO internally validates the model. This process ensures that the model isn't just memorizing the training data but is generalizing well to unseen examples. This process will also download the FashionMnist dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AchHMDLOa6w",
        "outputId": "83d642cb-50a6-4059-8a2c-21971209821d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.49 ğŸš€ Python-3.10.12 torch-2.5.1+cpu CPU (Intel Xeon 2.00GHz)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=yolov8n-cls.yaml, data=fashion-mnist, epochs=10, time=None, patience=100, batch=16, imgsz=28, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/train\n",
            "\n",
            "Dataset not found âš ï¸, missing path /content/datasets/fashion-mnist, attempting download...\n",
            "Downloading https://ultralytics.com/assets/fashion-mnist.zip to '/content/datasets/fashion-mnist.zip'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47.0M/47.0M [00:00<00:00, 159MB/s]\n",
            "Unzipping /content/datasets/fashion-mnist.zip to /content/datasets/fashion-mnist...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 70023/70023 [00:07<00:00, 9430.05file/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset download success âœ… (9.4s), saved to \u001b[1m/content/datasets/fashion-mnist\u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/datasets/fashion-mnist/train... found 60000 images in 10 classes âœ… \n",
            "\u001b[34m\u001b[1mval:\u001b[0m None...\n",
            "\u001b[34m\u001b[1mtest:\u001b[0m /content/datasets/fashion-mnist/test... found 10000 images in 10 classes âœ… \n",
            "Overriding model.yaml nc=1000 with nc=10\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    343050  ultralytics.nn.modules.head.Classify         [256, 10]                     \n",
            "YOLOv8n-cls summary: 99 layers, 1,451,098 parameters, 1,451,098 gradients, 3.4 GFLOPs\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/classify/train', view at http://localhost:6006/\n",
            "WARNING âš ï¸ imgsz=[28] must be multiple of max stride 32, updating to [32]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/fashion-mnist/train... 60000 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60000/60000 [00:16<00:00, 3656.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/datasets/fashion-mnist/train.cache\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/fashion-mnist/test... 10000 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:02<00:00, 4013.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/datasets/fashion-mnist/test.cache\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 26 weight(decay=0.0), 27 weight(decay=0.0005), 27 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
            "Image sizes 32 train, 32 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/classify/train\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       1/10         0G      2.308         16         32:   0%|          | 3/3750 [00:00<09:44,  6.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       1/10         0G       2.34         16         32:   0%|          | 7/3750 [00:00<08:29,  7.35it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755k/755k [00:00<00:00, 10.1MB/s]\n",
            "       1/10         0G      1.449         16         32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [07:46<00:00,  8.05it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:16<00:00, 19.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.723      0.994\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       2/10         0G     0.9405         16         32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [06:44<00:00,  9.28it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:16<00:00, 19.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.793      0.996\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       3/10         0G     0.8196         16         32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [06:43<00:00,  9.29it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:17<00:00, 18.30it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       0.82      0.997\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       4/10         0G     0.7318         16         32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [06:41<00:00,  9.35it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:17<00:00, 18.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.855      0.998\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       5/10         0G     0.6709         16         32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [06:40<00:00,  9.37it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:16<00:00, 18.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.858      0.998\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       6/10         0G     0.6289         16         32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [06:25<00:00,  9.73it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:16<00:00, 18.96it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.868      0.998\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       7/10         0G     0.5936         16         32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [06:34<00:00,  9.50it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:17<00:00, 18.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.863      0.998\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       8/10         0G      0.564         16         32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [06:41<00:00,  9.33it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:16<00:00, 18.45it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.876      0.999\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       9/10         0G     0.5426         16         32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [06:31<00:00,  9.58it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:16<00:00, 19.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.873      0.999\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      10/10         0G     0.5207         16         32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [06:26<00:00,  9.70it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:16<00:00, 19.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       0.88      0.999\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "10 epochs completed in 1.169 hours.\n",
            "Optimizer stripped from runs/classify/train/weights/last.pt, 3.0MB\n",
            "Optimizer stripped from runs/classify/train/weights/best.pt, 3.0MB\n",
            "\n",
            "Validating runs/classify/train/weights/best.pt...\n",
            "WARNING âš ï¸ validating an untrained model YAML will result in 0 mAP.\n",
            "Ultralytics 8.3.49 ğŸš€ Python-3.10.12 torch-2.5.1+cpu CPU (Intel Xeon 2.00GHz)\n",
            "YOLOv8n-cls summary (fused): 73 layers, 1,447,690 parameters, 0 gradients, 3.3 GFLOPs\n",
            "WARNING âš ï¸ Dataset 'split=val' not found, using 'split=test' instead.\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/datasets/fashion-mnist/train... found 60000 images in 10 classes âœ… \n",
            "\u001b[34m\u001b[1mval:\u001b[0m None...\n",
            "\u001b[34m\u001b[1mtest:\u001b[0m /content/datasets/fashion-mnist/test... found 10000 images in 10 classes âœ… \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:15<00:00, 20.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       0.88      0.999\n",
            "Speed: 0.0ms preprocess, 1.0ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/classify/train\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#Train the model\n",
        "results = model.train(data=\"fashion-mnist\", epochs=10, imgsz=28)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LRnIrQPlYJk"
      },
      "source": [
        "### Running Inference on the model Using some samples from the test data\n",
        "Defined the class labels for Fashion MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "a4XjU_OVh5tV"
      },
      "outputs": [],
      "source": [
        "# Define the manual class labels for Fashion MNIST\n",
        "class_names = [\n",
        "    \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
        "    \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle Boot\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z27t9TYRh8XW",
        "outputId": "2e595e48-ff02-4330-fc8f-e71dad41dbd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "image 1/1 /content/datasets/fashion-mnist/test/9/1007.png: 32x32 9 0.95, 7 0.05, 5 0.00, 8 0.00, 3 0.00, 2.7ms\n",
            "Speed: 4.7ms preprocess, 2.7ms inference, 0.0ms postprocess per image at shape (1, 3, 32, 32)\n",
            "Results saved to \u001b[1mruns/classify/train2\u001b[0m\n",
            "Predicted class: Ankle Boot, Confidence: 0.95\n"
          ]
        }
      ],
      "source": [
        "# Inference for a specific image\n",
        "results = model.predict(source=\"/content/datasets/fashion-mnist/test/9/1007.png\", save=True)\n",
        "\n",
        "# Process the results\n",
        "for result in results:\n",
        "    if hasattr(result, \"probs\") and result.probs is not None:\n",
        "        # Get the index of the top predicted class\n",
        "        predicted_class_idx = result.probs.top1\n",
        "        # Map the class index to the class name using the previously defined list\n",
        "        predicted_class_name = class_names[predicted_class_idx]\n",
        "        # Get the confidence score for the top class\n",
        "        confidence = result.probs.top1conf.item()\n",
        "\n",
        "        print(f\"Predicted class: {predicted_class_name}, Confidence: {confidence:.2f}\")\n",
        "    else:\n",
        "        print(\"No predictions for this image.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrTe5JVxiD3f",
        "outputId": "72861875-bea6-41ca-fb1a-c382166ba06b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "image 1/1 /content/datasets/fashion-mnist/test/0/1049.png: 32x32 0 0.93, 6 0.06, 3 0.00, 2 0.00, 4 0.00, 2.7ms\n",
            "Speed: 1.2ms preprocess, 2.7ms inference, 0.1ms postprocess per image at shape (1, 3, 32, 32)\n",
            "Results saved to \u001b[1mruns/classify/train3\u001b[0m\n",
            "Predicted class: T-shirt/top, Confidence: 0.93\n"
          ]
        }
      ],
      "source": [
        "# Inference for a more images\n",
        "results = model.predict(source=\"/content/datasets/fashion-mnist/test/0/1049.png\", save=True)\n",
        "\n",
        "# Process the results\n",
        "for result in results:\n",
        "    if hasattr(result, \"probs\") and result.probs is not None:\n",
        "        # Get the index of the top predicted class\n",
        "        predicted_class_idx = result.probs.top1\n",
        "        # Map the class index to the class name using the previously defined list\n",
        "        predicted_class_name = class_names[predicted_class_idx]\n",
        "        # Get the confidence score for the top class\n",
        "        confidence = result.probs.top1conf.item()\n",
        "        print(f\"Predicted class: {predicted_class_name}, Confidence: {confidence:.2f}\")\n",
        "    else:\n",
        "        print(\"No predictions for this image.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HC5mNmi0nB2g",
        "outputId": "60df52e3-c387-449a-d834-57aabc3f6379"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "image 1/1 /content/datasets/fashion-mnist/test/7/1040.png: 32x32 7 1.00, 5 0.00, 9 0.00, 8 0.00, 0 0.00, 2.6ms\n",
            "Speed: 1.0ms preprocess, 2.6ms inference, 0.0ms postprocess per image at shape (1, 3, 32, 32)\n",
            "Results saved to \u001b[1mruns/classify/train4\u001b[0m\n",
            "Predicted class: Sneaker, Confidence: 1.00\n"
          ]
        }
      ],
      "source": [
        "# Inference for a more images\n",
        "results = model.predict(source=\"/content/datasets/fashion-mnist/test/7/1040.png\", save=True)\n",
        "\n",
        "# Process the results\n",
        "for result in results:\n",
        "    if hasattr(result, \"probs\") and result.probs is not None:\n",
        "        # Get the index of the top predicted class\n",
        "        predicted_class_idx = result.probs.top1\n",
        "        # Map the class index to the class name using the previously defined list\n",
        "        predicted_class_name = class_names[predicted_class_idx]\n",
        "        # Get the confidence score for the top class\n",
        "        confidence = result.probs.top1conf.item()\n",
        "        print(f\"Predicted class: {predicted_class_name}, Confidence: {confidence:.2f}\")\n",
        "    else:\n",
        "        print(\"No predictions for this image.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPJUMWnBhb21",
        "outputId": "3b8a4726-f410-4878-fd6c-0ea52b487655"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "image 1/1 /content/datasets/fashion-mnist/test/8/1564.png: 32x32 8 1.00, 4 0.00, 0 0.00, 6 0.00, 3 0.00, 2.7ms\n",
            "Speed: 1.0ms preprocess, 2.7ms inference, 0.0ms postprocess per image at shape (1, 3, 32, 32)\n",
            "Results saved to \u001b[1mruns/classify/train5\u001b[0m\n",
            "Predicted class: Bag, Confidence: 1.00\n"
          ]
        }
      ],
      "source": [
        "# Inference for a more images\n",
        "results = model.predict(source=\"/content/datasets/fashion-mnist/test/8/1564.png\", save=True)\n",
        "\n",
        "# Process the results\n",
        "for result in results:\n",
        "    if hasattr(result, \"probs\") and result.probs is not None:\n",
        "        # Get the index of the top predicted class\n",
        "        predicted_class_idx = result.probs.top1\n",
        "        # Map the class index to the class name using the previously defined list\n",
        "        predicted_class_name = class_names[predicted_class_idx]\n",
        "        # Get the confidence score for the top class\n",
        "        confidence = result.probs.top1conf.item()\n",
        "        print(f\"Predicted class: {predicted_class_name}, Confidence: {confidence:.2f}\")\n",
        "    else:\n",
        "        print(\"No predictions for this image.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu5HMYtJ0LKD"
      },
      "source": [
        "### Testing on Google Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRwBe06LxdWy",
        "outputId": "58f34114-aedb-4981-853f-15704cbf43e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 640x640 6 0.31, 8 0.17, 0 0.12, 4 0.11, 2 0.11, 35.3ms\n",
            "Speed: 0.0ms preprocess, 35.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/classify/train6\u001b[0m\n",
            "Predicted class: Shirt, Confidence: 0.31\n"
          ]
        }
      ],
      "source": [
        "# Using an image from the web\n",
        "image_url = \"https://images.unsplash.com/photo-1521572163474-6864f9cf17ab?q=80&w=1780&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D\"\n",
        "\n",
        "# Download the image from the URL\n",
        "response = requests.get(image_url)\n",
        "img = Image.open(BytesIO(response.content))\n",
        "\n",
        "# Preprocessing the image for YOLO\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((640, 640)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: x if x.shape[0] == 3 else x.repeat(3, 1, 1))\n",
        "])\n",
        "img_tensor = transform(img).unsqueeze(0)\n",
        "\n",
        "# Perform inference\n",
        "results = model.predict(source=img_tensor, save=True)\n",
        "\n",
        "# Process the results\n",
        "for result in results:\n",
        "    if hasattr(result, \"probs\") and result.probs is not None:\n",
        "        predicted_class_idx = result.probs.top1\n",
        "        predicted_class_name = class_names[predicted_class_idx]\n",
        "        confidence = result.probs.top1conf.item()\n",
        "        print(f\"Predicted class: {predicted_class_name}, Confidence: {confidence:.2f}\")\n",
        "    else:\n",
        "        print(\"No predictions for this image.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkVmDaId5rBb"
      },
      "source": [
        "# **STYLING ADVICE PORTION**\n",
        "###**PARSING THE IMAGE TO AN LLM TO GIVE STYLING ADVICE**\n",
        "The LLM chosen is OpenAI 's ChatGPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0v1bqnLS1_d"
      },
      "source": [
        "###Testing my API and GPT 4's advice to see if the advice is suitable for the user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHseZZtT6Frw",
        "outputId": "8204437c-78fa-4a4c-b657-aa486a099e5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Casual Chic: A pair of ankle boots with skinny jeans, a plain white t-shirt and a lightweight cardigan. Accessorise with a chunky necklace or scarf for some added flare.\n",
            "\n",
            "2. Boho Vibe: Pair the ankle boots with a flowy maxi dress and a wide brimmed hat. This style works great for summer festivals or outdoor parties.\n",
            "\n",
            "3. Business Casual: Wear your ankle boots with a knee-length pencil skirt and a fitted blouse. This look would be appropriate for a work environment while still being fashionable and comfortable. A structured jacket can be added for an extra polished finish.\n",
            "\n",
            "4. Edgy Look: Black or metallic ankle boots combined with leather pants and a band t-shirt can make for a chic, edgy outfit. Top it off with a bomber jacket for a cool, casual look. \n",
            "\n",
            "5. Elegant Ensemble: Try wearing your ankle boots with a midi dress and a sleek, leather jacket for an evening out. Add a splash of color to the ensemble with a bright clutch or statement jewelry.\n",
            "\n",
            "6. Cozy Winter Outfit: Ankle boots look great with a chunky oversized sweater and leggings. Throw on a large scarf and beanie for extra warmth and style.\n",
            "\n",
            "7. Fall Fashion: Pair your ankle boots with a pair of slim fitting jeans, a fitted turtleneck and a long, sleeveless vest. The layers make it perfect for fall.\n",
            "\n",
            "8. Casual Day Out: Wear your boots with an oversized button-down shirt and leggings. This is a very comfortable outfit that also looks stylish and put together.\n",
            "\n",
            "9. Bold and Bright: Pair your ankle boots with a colorful midi skirt and a matching blouse. The bright colours can make a statement and stand out.\n",
            "\n",
            "10. Sophisticated and Classy: Try pairing the boots with high waisted trousers and a silk blouse. This ensemble is sure to make an impression.\n",
            " \n",
            "Remember, the best thing about ankle boots is their versatility, so don't be afraid to play around with styles to find what works best for you.\n"
          ]
        }
      ],
      "source": [
        "# Set your API key\n",
        "client =OpenAI(api_key=\"INSERT-API-KEY\")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-4\",\n",
        "  messages=[\n",
        "      {'role': 'user',\n",
        "       'content': 'You are a fashion expert. Suggest stylish outfit combinations for someone wearing an ankle boot.'}\n",
        "  ]\n",
        ")\n",
        "\n",
        "message = response.choices[0].message.content\n",
        "print(message)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZvtfkYLS_DK"
      },
      "source": [
        "### Trying to test the code to give fashion advice on a google image after the url has been inputed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4MNoYk-IJvn",
        "outputId": "5aa89ef7-ee76-4b99-9d11-4f5d18facf3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 640x640 6 0.31, 8 0.17, 0 0.12, 4 0.11, 2 0.11, 24.0ms\n",
            "Speed: 0.0ms preprocess, 24.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/classify/train7\u001b[0m\n",
            "Predicted class: Shirt, Confidence: 0.31\n",
            "1. Classic combination: A shirt with a well-fitted suit and tie, paired with a pair of brogues or loafers. Add some elegance with a pocket square and a sleek watch. \n",
            "\n",
            "2. Casual Day Outfit: A plaid or patterned shirt paired with slim-fit jeans and white sneakers. Top up the look with a cap or minimalist watch for a relaxing meetup.\n",
            "\n",
            "3. Office Attire: A crisp white shirt with navy or black trousers. Pair it with oxford shoes or derby shoes and a leather belt. Add a tie for a formal meeting.\n",
            "\n",
            "4. Street Style: A denim shirt layered over a basic t-shirt, paired with skinny jeans or ripped jeans and high-top sneakers. Finish the look with a snapback cap and a backpack. \n",
            "\n",
            "5. Summer Vibes: A printed short-sleeve shirt with cotton shorts and espadrilles or sandals. Accessorize with a straw hat and sunglasses.\n",
            "\n",
            "6. Sophisticated Casual: A pastel color shirt tucked into chino pants, paired with loafers or casual leather shoes. A leather belt and a pair of aviators would complete the look.\n",
            "\n",
            "7. Winter Look: A flannel shirt layered over a turtleneck, worn with a pair of straight-fit jeans and boots. Add a parka or overcoat based on the weather.\n",
            "\n",
            "8. Preppy Look: A striped shirt with a pair of khakis, paired with classic loafers. Layer a cardigan or sweater vest over the shirt for that extra preppy touch.\n",
            "  \n",
            "9. Edgy Look: A black shirt with black skinny jeans, paired with black boots. Add a leather jacket and a chunky metal watch to nail the edgy vibe.\n",
            "\n",
            "10. Bohemian Style: A loose, patterned shirt worn with wide-legged trousers and sandals. Accessorize with a headband and statement jewelry for a bohemian aesthetic.\n"
          ]
        }
      ],
      "source": [
        "# Using an image from the web\n",
        "image_url = \"https://images.unsplash.com/photo-1521572163474-6864f9cf17ab?q=80&w=1780&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D\"\n",
        "\n",
        "# Download the image from the URL\n",
        "response = requests.get(image_url)\n",
        "img = Image.open(BytesIO(response.content))\n",
        "\n",
        "# Preprocessing the image for YOLO\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((640, 640)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: x if x.shape[0] == 3 else x.repeat(3, 1, 1))\n",
        "])\n",
        "img_tensor = transform(img).unsqueeze(0)\n",
        "\n",
        "# Perform inference\n",
        "results = model.predict(source=img_tensor, save=True)\n",
        "\n",
        "# Process the results\n",
        "for result in results:\n",
        "    if hasattr(result, \"probs\") and result.probs is not None:\n",
        "        predicted_class_idx = result.probs.top1\n",
        "        predicted_class_name = class_names[predicted_class_idx]\n",
        "        confidence = result.probs.top1conf.item()\n",
        "        print(f\"Predicted class: {predicted_class_name}, Confidence: {confidence:.2f}\")\n",
        "    else:\n",
        "        print(\"No predictions for this image.\")\n",
        "\n",
        "# Set your API key\n",
        "client =OpenAI(api_key=\"INSERT-API-KEY\")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[\n",
        "        {'role': 'user',\n",
        "         'content': f'You are a fashion expert. Suggest stylish outfit combinations for someone wearing a {predicted_class_name}.'}\n",
        "    ]\n",
        ")\n",
        "\n",
        "message = response.choices[0].message.content\n",
        "print(message)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lG0Ohi_oTIrj"
      },
      "source": [
        "### Giving fashion advice after the user uploads their own image\n",
        "Will be uploading the image of a bag i found online"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "_j2TFYWOymr3",
        "outputId": "f763f783-f308-4373-f756-624e94a7585b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d5eb5fce-c3d3-44e3-ad5e-841dd6634dc9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d5eb5fce-c3d3-44e3-ad5e-841dd6634dc9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving a bag.jpg to a bag.jpg\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Upload the image\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljDdWZsoIJbR",
        "outputId": "469857c6-061f-4590-ca35-e9ae471880c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 640x640 8 0.45, 6 0.18, 0 0.14, 3 0.07, 2 0.06, 22.2ms\n",
            "Speed: 0.0ms preprocess, 22.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/classify/train8\u001b[0m\n",
            "Predicted class: Bag, Confidence: 0.45\n",
            "1. **Casual Chic**\n",
            "   - Bag: Leather tote bag in a neutral color\n",
            "   - Outfit: High waist skinny jeans, simple white t-shirt\n",
            "   - Shoes: Loafers or white sneakers\n",
            "   - Accessories: Gold hoop earrings, sunglasses \n",
            "\n",
            "2. **Boho Outfit**\n",
            "   - Bag: Fringe crossbody bag\n",
            "   - Outfit: Flowy chic maxi dress in floral prints\n",
            "   - Shoes: Gladiator sandals\n",
            "   - Accessories: Wide-brimmed hat, layered necklaces \n",
            "\n",
            "3. **Business Casual**\n",
            "   - Bag: Structured handbag in a bold color\n",
            "   - Outfit: Black pencil skirt, button-down white blouse\n",
            "   - Shoes: Black pointed heels\n",
            "   - Accessories: Thin silver watch, stud earrings  \n",
            "\n",
            "4. **Street Style**\n",
            "   - Bag: Small backpack in bright or pastel colors\n",
            "   - Outfit: Ripped jeans, oversized graphic tee or crop top\n",
            "   - Shoes: Chunky sneakers\n",
            "   - Accessories: Bucket hat, hoop earrings\n",
            "\n",
            "5. **Classic Elegance**\n",
            "   - Bag: Classic Chanel quilted bag\n",
            "   - Outfit: Little black dress\n",
            "   - Shoes: Black or nude pumps\n",
            "   - Accessories: Pearl earrings, diamond bracelet\n",
            "\n",
            "6. **Summer Outfit**\n",
            "   - Bag: Rattan woven bag or straw tote\n",
            "   - Outfit: Simple white sundress or shorts with a flowy top \n",
            "   - Shoes: Espadrilles or slip-on sandals\n",
            "   - Accessories: Sun hat, colorful statement earrings\n",
            "\n",
            "7. **Fall Charm**\n",
            "   - Bag: Leather satchel in brown or burgundy\n",
            "   - Outfit: Skinny jeans, cozy oversized sweater in autumn colors\n",
            "   - Shoes: Suede ankle boots\n",
            "   - Accessories: Blanket scarf, gold dangle earrings\n",
            "\n",
            "8. **Night Party**\n",
            "   - Bag: Sleek metallic clutch\n",
            "   - Outfit: Sexy sequins or satin dress\n",
            "   - Shoes: Strappy high heels\n",
            "   - Accessories: Statement necklace, diamond studs\n",
            "\n",
            "9. **Winter Wonderland**\n",
            "   - Bag: Faux fur or suede handbag\n",
            "   - Outfit: Plaid wool coat, skinny jeans, cozy sweater\n",
            "   - Shoes: Leather knee-high boots\n",
            "   - Accessories: Knit beanie hat, gloves\n",
            "\n",
            "10. **Sporty Look**\n",
            "   - Bag: Nylon fanny pack \n",
            "   - Outfit: Leggings, sports bra and a jacket \n",
            "   - Shoes: Running shoes\n",
            "   - Accessories: Sports watch, sunglasses.\n"
          ]
        }
      ],
      "source": [
        "# Get the uploaded file path\n",
        "image_path = list(uploaded.keys())[0]\n",
        "\n",
        "# Open the uploaded image\n",
        "img = Image.open(image_path)\n",
        "\n",
        "# Step 2: Preprocessing the image for YOLO\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((640, 640)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: x if x.shape[0] == 3 else x.repeat(3, 1, 1))\n",
        "])\n",
        "\n",
        "img_tensor = transform(img).unsqueeze(0)\n",
        "\n",
        "# Step 3: YOLO Prediction (Assuming 'model' is defined already)\n",
        "results = model.predict(source=img_tensor, save=True)\n",
        "\n",
        "# Step 4: Process the results\n",
        "for result in results:\n",
        "    if hasattr(result, \"probs\") and result.probs is not None:\n",
        "        predicted_class_idx = result.probs.top1\n",
        "        predicted_class_name = class_names[predicted_class_idx]\n",
        "        confidence = result.probs.top1conf.item()\n",
        "        print(f\"Predicted class: {predicted_class_name}, Confidence: {confidence:.2f}\")\n",
        "    else:\n",
        "        print(\"No predictions for this image.\")\n",
        "\n",
        "# Step 5: Ask OpenAI for fashion advice based on the predicted class\n",
        "client =OpenAI(api_key=\"INSERT-API-KEY\")\n",
        "\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[{\n",
        "        'role': 'user',\n",
        "        'content': f'You are a fashion expert. Suggest stylish outfit combinations for someone wearing a {predicted_class_name}.'}]\n",
        ")\n",
        "\n",
        "message = response.choices[0].message.content\n",
        "print(message)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
